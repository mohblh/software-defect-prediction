\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Rapport de Projet : Machine Learning et Sélection de Caractéristiques}
\author{Nom de l'équipe}
\date{\today}

\begin{document}

\maketitle

\section{Introduction au Machine Learning}
Le machine learning (apprentissage automatique) est une branche de l'intelligence artificielle permettant aux systèmes d'apprendre à partir de données pour effectuer des prédictions ou des classifications sans programmation explicite.

\subsection{Distinction entre apprentissage supervisé et non supervisé}
- **Supervisé** : Utilise des données étiquetées pour entraîner un modèle. Exemples : classification (prédire des catégories) et régression (prédire des valeurs continues).  
- **Non supervisé** : Travaille avec des données non étiquetées pour identifier des structures ou regroupements. Exemple : clustering.

\subsection{Classification et régression linéaire}
- **Classification** : Prédit des classes (ex. oui/non). Exemples : SVM, KNN.  
- **Régression linéaire** : Prédit une valeur numérique en supposant une relation linéaire entre les variables.

\section{Répartition des Algorithmes Étudiés}
Nous avons étudié 8 algorithmes, répartis comme suit :
- **Khadidja** : Régression (supervisée), ACP (non supervisée).  
- **Khawla** : Arbre de décision (supervisé), Clustering hiérarchique (non supervisé).  
- **Moh** : KNN (supervisé), K-Means (non supervisé).  
- **Yacine** : SVM (supervisé), Forêt aléatoire (supervisé).

\section{Compréhension du Principe de Feature Selection}
### Pourquoi c'est important
- Réduit la complexité en éliminant les caractéristiques inutiles.  
- Améliore la précision en évitant le surapprentissage.  
- Réduit les coûts computationnels.

### Méthodes étudiées
- **Lasso (L1 Regularization)** : Pénalise les caractéristiques moins importantes.  
- **Statistiques/Score basé** : Sélectionne les meilleures caractéristiques selon des scores.  
- **RFE (Recursive Feature Elimination)** : Élimine récursivement les caractéristiques faibles.  
- **Feature Importance** : Utilise l'importance calculée des caractéristiques.  
- **Chi-Square** : Évalue la dépendance entre caractéristiques et cible.

\section{Tests de Performance et Résultats}
Nous avons testé les algorithmes suivants avec la sélection de caractéristiques :
- **Algorithmes** : KNN, SVM, Régression logistique, Forêt aléatoire.  
- **Résultats** : La précision a augmenté après la sélection (ex. jusqu'à 97\%).

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Algorithme} & \textbf{Précision sans FS} & \textbf{Précision avec FS} \\
        \hline
        Régression logistique & 85\% & 92\% \\
        \hline
        KNN & 88\% & 95\% \\
        \hline
        SVM & 90\% & 97\% \\
        \hline
        Forêt aléatoire & 91\% & 97\% \\
        \hline
    \end{tabular}
    \caption{Résultats de précision}
    \label{tab:results}
\end{table}

\section{Diagrammes de Processus}
\subsection{Flow Diagram}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{flow_diagram.png}
    \caption{Flow Diagram général du processus}
    \label{fig:flow_diagram}
\end{figure}
Étapes : Entrée des données → Prétraitement → Feature Selection → Splitting → Classification → Prédiction.

\subsection{Modèle de Formation/Validation}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{training_validation.png}
    \caption{Modèle de formation et validation}
    \label{fig:training_validation}
\end{figure}
Étapes : Préparation des données → Choix des algorithmes → Validation croisée.

\section{Conclusion et Prochaines Étapes}
**Résultats** : Précision atteignant ~97\% grâce à la sélection de caractéristiques.  
**Prochaines pistes** : Tester d'autres méthodes (ex. Elastic Net), comparer d'autres métriques (F1-score), finaliser le rapport.

\end{document}